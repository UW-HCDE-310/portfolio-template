<!DOCTYPE HTML>
<!--
	Alpha by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Naomi, UX Researcher</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<header id="header">
					<h1><a href="index.html">Naomi, UX researcher</a></h1>
					<nav id="nav">
						<ul>
							<li>
								<a href="#" class="icon solid fa-angle-down">Projects</a>
								<ul>
									<li><a href="mentor.html">Mentor - search & request</a></li>
									<li><a href="jdict.html">Jdict - language tools</a></li>
									<li><a href="poirot.html">Poirot - dev tools</a></li>
									<li><a href="juxxt.html">Juxxt - design tools</a></li>
									<li><a href="vat.html">VAT - annotation tools</a></li>
								</ul>
							</li>
						</ul>
					</nav>
				</header>

			<!-- Main -->
				<section id="main" class="container">
					<header>
						<h2>Video Annotation Tool (VAT)</h2>
						<p>Simplifying data labeling for machine learning</p>
					</header>
					<div class="box">
						<h3>Summary</h3>
						<p>Data from gyroscopes and accelerometers often has multiple axises and can be difficult to read, but having a video that shows the sensor does not always make annotators more accurate or efficient. I designed a study to investigate why this might be the case, ran 60 users through the study, and wrote scripts to analyze quantitive data about how users used our Video Annotation Tool (VAT) and their annotations. 
						<p>Timeline: November 2018-April 2020</p>
						<p>Team: Lawrence Thatcher, Dr. Michael Jones, and Dr. Kevin Seppi</p>
						<h3>Outline</h3>
						<ol>
							<li>The problem</li>
							<li>Background research</li>
							<li>User studies</li>
							<li>Outcome and impact</li>
						</ol>
						<h3>The problem</h3>
						<p>Training classifiers for human activity recognition systems often relies on large corpora of annotated sensor data. Crowd sourcing is one way to collect and annotate large amounts of sensor data, but is often completed by unskilled or inexperienced workers. Non-expert annotators sometimes struggle with: 1) data acquisition and annotation, 2) annotating reliably, and 3) inaccurate results that do not yield generalizable event classifiers. </p>

						<h3>Background research</h3>
						<p>My team created software that allowed users to annotate events as they watch data and video. This screenshot shows a user waiting for the next "run" or "skip" event as they annotate data for a specialized pedometer for athletes. We used this interface, as well as a version without video and version without data.</p>
						<p>To my team's surprise, we found that having video did not always make users more accurate/efficient.</p>
						<span class="image featured"><img src="images/vat/vat_gui.png" alt=""/></span>
						<h3>User studies</h3>
						<p>For this project I designed, ran, and evaluated data from an IRB-approved study to explore whether users could collect video and data, as well as produce annotations suitable for machine learning.</p>

						<h3>Outcome and impact</h3>
						<p>We found that adding synchronized video to sensor data for labeling creates a more intuitive labeling experience and supports increased precision in labeling. This approach allows designers to quickly create machine learning models for physical interactive devices which must recognize events in the physical world. We have run several user studies to document behaviour of non-expert users in the context of annotating physical event and to demonstrate the advantages of our approach.
						</p>
						<p>I am second author on our <a href="https://dl.acm.org/doi/abs/10.1145/3267305.3267507">UbiComp '18 paper</a> and first author on our chapter in the book <a href="https://link.springer.com/chapter/10.1007/978-3-030-13001-5_7">Human Activity Sensing</a> which explain our original work. A third paper is in the works, which elaborates on when video is not beneficial for data annotation.
						</p>
					</div>
				</section>

				<!-- Footer -->
				<section id="footer">
					<ul class="icons">
						<li><a href="https://www.linkedin.com/in/naomi789/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
						<li><a href="https://www.github.com/naomi789" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li><a href="https://www.instagram.com/naomi.johndaughter/" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
						<!-- <li><a href="https://www.naomijohndaughter.com" class="icon brands fa-camera"><span class="label">Photography</span></a></li> -->
						<li><a href="#snjohnson789@gmail.com" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
					</ul>
					<p class="copyright">&copy; 2020 <a href="index.html">Naomi Johnson</a>.
	  					Design: <a href="http://html5up.net">HTML5 UP</a> <a href="https://html5up.net/alpha">Alpha</a>.
					</p>
				</section>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
